class(dhoni_df)
dhoni_list[[1]]
dhoni_df[[1]]
dhoni_df <- bind_rows(lapply(dhoni_list, as.data.frame))
library(dplyr)
library(lubridate)
library(stringr)
library(tm)
dhoni_df <- bind_rows(lapply(dhoni_list, as.data.frame))
dhoni_df[[1]]
class(dhoni_df)
names(dhoni_df)
dhoni_df$text[[1]]
dhoni_df$retweeted
dhoni_df$retweetCount
dhoni_df <- lapply(dhoni_list, as.data.frame)
class(dhoni_df)
names(dhoni_df)
dim(dhoni_df)
dhoni_df[1]
dhoni_df[[1]]
dhoni_df[[2]]
dhoni_df[[1500]]
dhoni_df <- bind_rows(dhoni_df)
dhoni_df$text
dhoni_df$text[1]
dhoni_df$date <- day(dhoni_df$created)
dhoni_df$hour <- hour(dhoni_df$created)
name(dhoni_df)
names(dhoni_df)
head(dhoni_df)
dhoni_df$text <- gsub("@[[:alpha:]]*","",dhoni_df$text)
dhoni_df$text[1]
dhoni_df$text[2]
text_corpus <- Corpus(VectorSource(dhoni_df$text))
class(text_corpus)
head(text_corpus)
text_corpus <- tm_map(text_corpus,tolower)
?removeWords
text_corpus <- tm_map(text_corpus, removeWords,
c("dhoni", "rt", "re", "amp"))
stopwords("english")
text_corpus <- tm_map(text_corpus, removeWords,
stopwords("english"))
text_corpus <- tm_map(text_corpus, removePunctuation)
text_df <- data.frame(text_clean = get("content", text_corpus),
stringsAsFactors = FALSE)
text_df$text_clean[1]
dhoni_df <- cbind.data.frame(dhoni_df,text_df)
install.packages("SentimentAnalysis")
library(twitteR)
library(SentimentAnalysis)
dhoni_sentiment <- analyzeSentiment(dhoni_df$text_clean)
head(dhoni_sentiment)
dhoni_sentiment <- dplyr::select(dhoni_sentiment,
SentimentGI, SentimentHE,
SentimentLM, SentimentQDAP,
WordCount)
head(dhoni_sentiment)
rowMeans(dhoni_sentiment[,-5])
dhoni_sentiment <- dplyr::mutate(dhoni_sentiment,
mean_sentiment = rowMeans(dhoni_sentiment[,-5]))
dhoni_sentiment <- dplyr::select(dhoni_sentiment,
WordCount,
mean_sentiment)
dhoni_df <- cbind.data.frame(dhoni_df, dhoni_sentiment)
head(dhoni_df)
davis_df_negative <- filter(davis_df, mean_sentiment < 0)
dhoni_df_negative <- filter(dhoni_df, mean_sentiment < 0)
nrow(dhoni_df_negative)
install.packages("quanteda")
library(quanteda)
dhoni_tokenized_list <- tokens(dhoni_df_negative$text)
head(dhoni_tokenized_list)
dhoni_dfm <- dfm(dhoni_tokenized_list)
head(dhoni_dfm)
word_sums <- colSums(dhoni_tokenized_list)
word_sums <- colSums(dhoni_dfm)
length(word_sums)
freq_data <- data.frame(words = names(word_sums), freq = word_sums, row.names = NULL,
stringsAsFactors = FALSE)
head(freq_data)
order(freq_data$freq,decreasing = TRUE)
freq_data <- freq_data[order(freq_data$freq,decreasing = TRUE),]
head(freq_data)
dhoni_corpus_tm <- corpus(VectorSource(dhoni_df_negative[,19]))
dhoni_df_negative[,19]
dhoni_df_negative$text_clean
dhoni_corpus_tm <- corpus(VectorSource(dhoni_df_negative$text_clean))
library(twitteR)
consumer_key <- "eAnyisrocHc70WM36lV19s4Oj"
consumer_secret <- "5zThdZxAybhOdypTpBtINtjC6O4c7NchPsb5f0MqHilBORluTt"
access_token <- "1319262410759237637-ro9T8KQCxSaHJrUqyBevO0Xyypbwx2"
access_secret <- "oWpZvc1wGUvi1LCh11kJchNeAtXcjMP853LCuFRZmmtXq"
setup_twitter_oauth(consumer_key, consumer_secret,
access_token, access_secret)
tweets <- searchTwitter("data science", n = 1000)
install.packages("ROAuth")
library(ROAuth)
library(dplyr)
install.packages("sentiment")
library(RCurl)
install.packages("syuzhet")
library(syuzhet)
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(httr)
library(wordcloud)
library(RCurl)
library(syuzhet)
consumer_key <- "eAnyisrocHc70WM36lV19s4Oj"
consumer_secret <- "5zThdZxAybhOdypTpBtINtjC6O4c7NchPsb5f0MqHilBORluTt"
access_token <- "1319262410759237637-ro9T8KQCxSaHJrUqyBevO0Xyypbwx2"
access_secret <- "oWpZvc1wGUvi1LCh11kJchNeAtXcjMP853LCuFRZmmtXq"
setup_twitter_oauth(consumer_key, consumer_secret,
access_token, access_secret)
tweets <- searchTwitter("data science", n = 1000)
tweets <- searchTwitter("data science", n = 1000, lang = "en")
length_tweets <- length(tweets)
length_tweets
?ldply
tweets_text <- sapply(tweets, function(x) x$getText())
tweets_text[[1]]
tweets_text[[1:2]]
tweets_text[1:2]
## Cleaning: remove people name, RT text etc..
tweets_text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)","",tweets_text)
tweets_text[1:2]
# remove html links
tweets_text <- gsub("http[^[:blank:]]+","",tweets_text)
tweets_text[1:2]
# remove people names
tweets_text <- gsub("@\\w+","",tweets_text)
tweets_text[1:2]
# remove punctuations
tweets_text <- gsub("[[:punct:]]","",tweets_text)
tweets_text[1:2]
tweets_text <- gsub("[[:alnum:]]","",tweets_text)
tweets[1:3]
tweets_text[1:3]
tweets_text <- gsub("[^[:alnum:]]","",tweets_text)
tweets_text[1:3]
tweets_text <- sapply(tweets, function(x) x$getText())
## Cleaning
# remove people name, RT text etc..
tweets_text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)","",tweets_text)
tweets_text[1:2]
# remove html links
tweets_text <- gsub("http[^[:blank:]]+","",tweets_text)
tweets_text[1:2]
# remove people names
tweets_text <- gsub("@\\w+","",tweets_text)
tweets_text[1:2]
# remove punctuations
tweets_text <- gsub("[[:punct:]]","",tweets_text)
tweets_text[1:2]
tweets_text <- gsub("[^[:alnum:]]","",tweets_text)
tweets_text[1:3]
# creating wordcorpus and cleaning
tweets_text <- Corpus(VectorSource(tweets_text))
tweets_text[1:2]
tweets_text <- tm_map(tweets_text, removePunctuation)
tweets_text <- tm_map(tweets_text, content_transformer(tolower))
tweets_text <- tm_map(tweets_text, removeWords, stopwords("english"))
tweets_text <- tm_map(tweets_text, stripWhitespace)
pal <- brewer.pal(8,"Dark2")
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE, color = pal)
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE, color = pal)
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE, colors = pal)
# creating wordcorpus and cleaning
tweets_text <- Corpus(VectorSource(tweets_text))
tweets_text[1:2]
tweets_text <- tm_map(tweets_text, removePunctuation)
tweets_text <- tm_map(tweets_text, content_transformer(tolower))
tweets_text <- tm_map(tweets_text, removeWords, stopwords("english"))
tweets_text <- tm_map(tweets_text, stripWhitespace)
pal <- brewer.pal(8,"Dark2")
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE, colors = pal)
tweets_text$`1`
tweets_text$[1]
tweets_text[1]
consumer_key <- "eAnyisrocHc70WM36lV19s4Oj"
consumer_secret <- "5zThdZxAybhOdypTpBtINtjC6O4c7NchPsb5f0MqHilBORluTt"
access_token <- "1319262410759237637-ro9T8KQCxSaHJrUqyBevO0Xyypbwx2"
access_secret <- "oWpZvc1wGUvi1LCh11kJchNeAtXcjMP853LCuFRZmmtXq"
setup_twitter_oauth(consumer_key, consumer_secret,
access_token, access_secret)
tweets <- searchTwitter("data science", n = 1000, lang = "en")
tweets <- searchTwitter("data science", n = 1500, lang = "en")
length(tweets)
tweets_text <- sapply(tweets, function(x) x$getText())
tweets_text[1:2]
## Cleaning
# remove people name, RT text etc..
tweets_text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)","",tweets_text)
tweets_text[1:2]
# remove html links
tweets_text <- gsub("http[^[:blank:]]+","",tweets_text)
tweets_text[1:2]
# remove people names
tweets_text <- gsub("@\\w+","",tweets_text)
tweets_text[1:2]
# remove punctuations
tweets_text <- gsub("[[:punct:]]","",tweets_text)
tweets_text[1:2]
tweets_text <- gsub("[^[:alnum:]]","",tweets_text)
tweets_text[1:3]
# creating wordcorpus and cleaning
tweets_text <- Corpus(VectorSource(tweets_text))
tweets_text[1:2]
tweets_text <- tm_map(tweets_text, removePunctuation)
tweets_text <- tm_map(tweets_text, content_transformer(tolower))
tweets_text <- tm_map(tweets_text, removeWords, stopwords("english"))
tweets_text <- tm_map(tweets_text, stripWhitespace)
pal <- brewer.pal(8,"Dark2")
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE, colors = pal)
tweets <- searchTwitter("trump", n = 1500, lang = "en")
tweets_text <- sapply(tweets, function(x) x$getText())
## Cleaning
# remove people name, RT text etc..
tweets_text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)","",tweets_text)
# remove html links
tweets_text <- gsub("http[^[:blank:]]+","",tweets_text)
# remove people names
tweets_text <- gsub("@\\w+","",tweets_text)
# remove punctuations
tweets_text <- gsub("[[:punct:]]","",tweets_text)
tweets_text <- gsub("[^[:alnum:]]","",tweets_text)
# creating wordcorpus and cleaning
tweets_text <- Corpus(VectorSource(tweets_text))
tweets_text[1:2]
tweets_text <- tm_map(tweets_text, removePunctuation)
tweets_text <- tm_map(tweets_text, content_transformer(tolower))
tweets_text <- tm_map(tweets_text, removeWords, stopwords("english"))
tweets_text <- tm_map(tweets_text, stripWhitespace)
pal <- brewer.pal(8,"Dark2")
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE, colors = pal)
wordcloud(tweets_text, min.freq = 5, max.words = Inf, width=1000, height=1000,
random.order = FALSE)
warnings()
term_matrix <- TermDocumentMatrix(tweets_text,
control = list(wordLengths = c(1,Inf)))
term_matrix
(frequency.terms <- findfrequencyTerms(term_matrix, lowfrequency = 20))
library(tm)
(frequency.terms <- findfrequencyTerms(term_matrix, lowfrequency = 20))
(frequency.terms <- findFreqTerms(term_matrix, lowfrequency = 20))
?findFreqTerms
(frequency.terms <- findFreqTerms(term_matrix, lowfrequency = 20))
frequency.terms <- findFreqTerms(term_matrix, lowfrequency = 20)
frequency.terms <- findFreqTerms(term_matrix)
term_matrix <- as.matrix(term_matrix)
term_matrix
head(term_matrix)
str(term_matrix)
dim(term_matrix)
term_matrix[1,]
term_matrix[1,1]
term_matrix[1,2]
term_matrix[11,2]
distinctWordsfrequency <- sort(rowSums(term_matrix), decreasing = T)
distinctWordsfrequency
names(distinctWordsfrequency)
distinctWordsDF <- data.frame(names(distinctWordsfrequency), distinctWordsfrequency)
distinctWordsDF
colnames(distinctWordsDF) <- c("word", "frequency")
distinctWordsDF
install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(distinctWordsDF)
tweets_df <- twListToDF(tweets)
(n.tweet <- length(tweets))
n.tweet
n.tweet <- length(tweets)
(n.tweet <- length(tweets))
tweets_text <- sapply(tweets_text, function(row) iconv(row, "latin1", "ASCII",
sub = "byte"))
functionalText = str_replace_all(tweets_text, "[^[:graph:]]", " ")
tweets_collections <- Corpus(VectorSource(tweets_unique))
tweets_collection <- Corpus(VectorSource(tweets_unique))
tweets_unique <- unique(tweets_text)
tweets_collection <- Corpus(VectorSource(tweets_unique))
functionalText = str_replace_all(tweets_collection, "[^[:graph:]]", " ")
tweets_collections <- tm_map(tweets_collections,
content_transformer(function(x)
iconv(x, to = "latin1", "ASCII", sub = "")))
# We change all words from capital to lower case
tweets_collections <- tm_map(tweets_collections, content_transformer(tolower))
tweets_collection <- tm_map(tweets_collection,
content_transformer(function(x)
iconv(x, to = "latin1", "ASCII", sub = "")))
# We change all words from capital to lower case
tweets_collection <- tm_map(tweets_collection, content_transformer(tolower))
# We delete all punctuations
tweets_collection <- tm_map(tweets_collection, removePunctuation)
# From tm package we use the below functions to return various kinds of
# stopwords with support for different languages.
tweets_collection <- tm_map(tweets_collection, function(x) removeWords(x,stopwords()))
tweets_collection <- tm_map(tweets_collection, removeWords, stopwords("english"))
tweets_collection <- tm_map(tweets_collection, removeNumbers)
tweets_collection <- tm_map(tweets_collection, stripWhitespace)
term_matrix <- TermDocumentMatrix(tweets_collections,
control = list(wordLengths = c(1,Inf)))
term_matrix <- TermDocumentMatrix(tweets_collection,
control = list(wordLengths = c(1,Inf)))
term_matrix
(frequency.terms <- findfrequencyTerms(term_matrix, lowfrequency = 20))
(frequency.terms <- findFreqTerms(term_matrix, lowfrequency = 20))
(frequency.terms <- findFreqTerms(term_matrix))
term_matrix <- as.matrix(term_matrix)
distinctWordsfrequency <- sort(rowSums(term_matrix), decreasing = T)
distinctWordsfrequency <- sort(rowSums(term_matrix), decreasing = T)
distinctWordsDF <- data.frame(names(distinctWordsfrequency), distinctWordsfrequency)
colnames(distinctWordsDF) <- c("word", "frequency")
wordcloud2(distinctWordsDF)
twitter.sub <- read.csv("https://raw.githubusercontent.com/HarshalSanap/Twitter-Text-Mining/master/twittersubset.csv")
View(twitter.sub)
class(twitter.sub)
library(twitteR)
library(dplyr)
library(lubridate)
library(stringr)
library(tm)
library(SentimentAnalysis)
consumer_key <- "eAnyisrocHc70WM36lV19s4Oj"
consumer_secret <- "5zThdZxAybhOdypTpBtINtjC6O4c7NchPsb5f0MqHilBORluTt"
access_token <- "1319262410759237637-ro9T8KQCxSaHJrUqyBevO0Xyypbwx2"
access_secret <- "oWpZvc1wGUvi1LCh11kJchNeAtXcjMP853LCuFRZmmtXq"
setup_twitter_oauth(consumer_key, consumer_secret,
access_token, access_secret)
tweets <- searchTwitter("Trump", n = 1500)
tweets_df <- twListToDF(tweets)
View(tweets_df)
View(unique(tweets_df))
View(tweets_df$text)
View(unique(tweets_df$text))
View(twitter.sub)
tweets_text_df <- tweets_df$text
tweets_text_df
head(tweets_text_df)
str(tweets_text_df)
class(tweets_text_df)
tweets_text_df <- as.character(tweets_df$text)
class(tweets_text_df)
head(tweets_text_df)
## remove letters, digits, and punctuation haracters starting with @
## remove usernames and replace with "USER"
tweets_text_df$text <- gsub("@\\w*"," USER", tweets_text_df)
# seperating text column and converting text to character
tweets_text_df <- tweets_df$text
View(tweets_text_df)
# seperating text column and converting text to character
tweets_text_df <- data.frame(Text = tweets_df$text)
View(tweets_text_df)
tweets_text_df$Text <- unique(tweets_text_df$Text)
# seperating text column and converting text to character
tweets_text_df <- tweets_df$text
View(unique(tweets_text_df))
tweets_text_df <- data.frame(Text=unique(tweets_text_df))
View(tweets_text_df)
## remove letters, digits, and punctuation haracters starting with @
## remove usernames and replace with "USER"
tweets_text_df$Text <- gsub("@\\w*"," USER", tweets_text_df)
View(tweets_text_df)
tweets <- searchTwitter("Trump", n = 1500)
tweets_df <- twListToDF(tweets)
# seperating text column and converting text to character
tweets_text <- tweets_df$text
tweets_text_df <- data.frame(Text=unique(tweets_text))
View(tweets_text_df)
## remove letters, digits, and punctuation haracters starting with @
## remove usernames and replace with "USER"
tweets_text_df$Text <- gsub("@\\w*"," USER", tweets_text_df$Text)
View(tweets_text_df)
##Remove website links and replace with "URL"
tweets_text_df$Text  <- gsub("http[[:alnum:][:punct:]]*"," WEBADDRESS",
tolower(tweets_text_df$Text ))
tweets_text_df$Text  <- gsub("www[[:alnum:][:punct:]]*"," WEBADDRESS",
tolower(tweets_text_df$Text ))
View(tweets_text_df)
tweets_text_df[1]
tweets_text_df$Text[1]
#remove html entitties like &quot; starting with
#note perfect but we will remove remaining punctation at later step
twitter.sub$Text<-gsub("\\&\\w*;","", twitter.sub$Text)
#remove html entitties like &quot; starting with
#note perfect but we will remove remaining punctation at later step
tweets_text_df$Text<-gsub("\\&\\w*;","", tweets_text_df$Text)
tweets_text_df$Text[1]
#remove any letters repeated more than twice (eg. hellooooooo -> helloo)
twitter.sub$Text  <- gsub('([[:alpha:]])\\1+', '\\1\\1', twitter.sub$Text)
#remove any letters repeated more than twice (eg. hellooooooo -> helloo)
tweets_text_df$Text  <- gsub('([[:alpha:]])\\1+', '\\1\\1', tweets_text_df$Text)
#additional cleaning removing leaving only letters numbers or spaces
tweets_text_df$Text <- gsub("[^a-zA-Z0-9 ]","",tweets_text_df$Text)
#review tweets now
head(tweets_text_df$Text,10)
Twitter_Corpus <- Corpus(VectorSource(tweets_text_df$Text))
Twitter_Corpus <- tm_map(Twitter_Corpus, stemDocument)
Twitter_Corpus <- tm_map(Twitter_Corpus, removePunctuation)
Twitter_Corpus <- tm_map(Twitter_Corpus, removeNumbers)
Twitter_Corpus <- tm_map(Twitter_Corpus, removeWords, stopwords("english"))
Twitter_Corpus <- tm_map(Twitter_Corpus, stripWhitespace)
#create term document matrix (terms as rows, documents as columns)
tdm <- TermDocumentMatrix(Twitter_Corpus)
#inspect the term document matrix, make sure to subset it is very large
inspect(tdm[1:30, 1:2])
tdm
View(tdm)
tdm$nrow
tdm$ncol
names(tdm$ncol)
dim(td)
dim(tdm)
# define tdm as matrix
m = as.matrix(tdm)
View(m)
?TermDocumentMatrix
v <- sort(rowSums(m),decreasing=TRUE)
View(v)
df <- data.frame(word = names(v),freq=v)
View(df)
wordcloud(words = df$word, freq = df$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud2(df)
barplot(df[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="steelblue", main ="Most frequent words",
ylab = "Word frequencies")
barplot(df[1:10,]$freq, las = 2, names.arg = df[1:10,]$word,
col ="steelblue", main ="Most frequent words",
ylab = "Word frequencies")
library(tidyr)
library(janeaustenr)
sentiments
mysentiment <- get_nrc_sentiment(df$word)
View(mysentiment)
class(mysentiment)
sentimentScores <- data.frame(colSums(mysentiment))
View(sentimentScores)
wordcloud2(sentimentScores)
sentimentScores <- data.frame(Score= colSums(mysentiment))
sentimentScores <- cbind(Sentiment = rownames(sentimentScores), sentimentScores)
View(sentimentScores)
wordcloud2(sentimentScores)
rownames(sentimentScores) <- NULL
View(sentimentScores)
library(tidytext)
sentiments
austen_books()
str_detect(text, regex("^chapter [\\divxlc]", ignore_case = T))))) %>%
ungroup() %>%
unnest_tokens(word, text)
library(janeaustenr)
library(stringr)
library(tidytext)
library(dplyr)
str_detect(text, regex("^chapter [\\divxlc]", ignore_case = T))))) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(), chapter = cumsum((
str_detect(text, regex("^chapter [\\divxlc]", ignore_case = T))))) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(), chapter = cumsum((
str_detect(text, regex("^chapter [\\divxlc]", ignore_case = T))))) %>%
ungroup() %>%
unnest_tokens(word, text)
get_sentiments("bing")
?sentiments
library(janeaustenr)
library(stringr)
library(tidytext)
library(dplyr)
tidy_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(), chapter = cumsum((
str_detect(text, regex("^chapter [\\divxlc]", ignore_case = T))))) %>%
ungroup() %>%
unnest_tokens(word, text)
austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(), chapter = cumsum((
str_detect(text, regex("^chapter [\\divxlc]", ignore_case = T)))))
?unnest_tokens
tidy_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
?inner_join
head(sentiments)
head(df)
df %>% inner_join(sentiments, by = word)
df %>% inner_join(sentiments, by = "word")
setwd("E:/R/Projects/Uber Data Analysis/figure")
